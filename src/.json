[
  {
    "commands": "Plan, Deploy and Cleanup Infrastructure",
    "text": "<p> terraform apply --auto-approve apply changes without being prompted to enter 'yes' </p> 
         <p> terraform destroy --auto-approve destroy/cleanup deployment without being prompted for 'yes'  </p> 
          <p>terraform plan -out plan.out output the deployment plan to plan.out </p>
          <p> terraform apply plan.out use the plan.out plan file to deploy infrastructure</p>
          <p> terraform plan -destroy outputs a destroy plan  </p>
          <p> terraform apply -target=aws_instance.my_ec2 only apply/deploy changes to the targeted resource</p>
          <p> terraform apply -target=aws_instance.my_ec2 only apply/deploy changes to the targeted resource </p>
          <p> terraform apply -lock=true lock the state file so it can’t be modified by any other Terraform apply or modification action(possible only where backend allows locking) </p>
          <p> terraform apply refresh=false do not reconcile state file with real-world resources(helpful with large complex deployments for saving deployment time) </p>
          <p> terraform apply --parallelism=5 number of simultaneous resource operations </p>
          <p> terraform refresh reconcile the state in Terraform state file with real-world resources  </p>
          <p> terraform providers get information about providers used in current configuration </p>"
  },
  {
    "commands": "Terraform Workspaces",
    "text": "<p> terraform workspace new mynewworkspace create a new workspace </p> 
         <p>terraform workspace select default change to the selected workspace </p>
          <p> terraform workspace list list out all workspaces </p> "
  },
  {
    "commands": "Terraform State Manipulation ",
    "text": "<p> terraform state show aws_instance.my_ec2 show details stored in Terraform state for the resource  </p> 
         <p> terraform state pull > terraform.tfstate download and output terraform state to a file </p> 
         <p> terraform state mv aws_iam_role.my_ssm_role module.custom_module move a resource tracked via state to different module </p> 
          <p> terraform state replace-provider hashicorp/aws registry.custom.com/aws replace an existing provider with another  </p>
           <p> terraform state list list out all the resources tracked via the current state file  </p>
            <p> terraform state rm  aws_instance.myinstace unmanage a resource, delete it from Terraform state file </p>"
  },
  {
    "commands": " Terraform Import And Outputs  ",
    "text": "<p> terraform import aws_instance.new_ec2_instance i-abcd1234 import EC2 instance with id i-abcd1234 into the Terraform resource named 'new_ec2_instance' of type 'aws_instance' </p> 
         <p> terraform import 'aws_instance.new_ec2_instance[
      0
    ]' i-abcd1234 same as above, imports a real-world resource into an instance of Terraform resource </p> 
         <p> terraform output list all outputs as stated in code   </p> 
         <p> terraform output instance_public_ip list out a specific declared output </p> 
         <p> terraform output instance_public_ip list out a specific declared output </p>
         <p> terraform output -json list all outputs in JSON format  </p>"
  },
  {
    "commands": "Terraform CLI tricks",
    "text": " <p> terraform -install-autocomplete Setup tab auto-completion, requires logging back in </p> 
        <p> Format and Validate Terraform code </p> 
        <p>  terraform fmt format code per HCL canonical standard  </p>
        <p> terraform validate validate code for syntax </p>
        <p> terraform validate -backend=false validate code skip backend validation   </p>
        <p> terraform init initialize directory, pull down providers </p>
        <p> terraform init -get-plugins=false initialize directory, do not download plugins </p>
        <p> terraform init -verify-plugins=false initialize directory, do not verify plugins for Hashicorp signature </p>"
  },
  {
    "commands": "Terraform Miscelleneous commands",
    "text": " <p> terraform version display Terraform binary version, also warns if version is old </p> 
        <p> terraform get -update=true download and update modules in the'root' module. </p> 
        <p> Terraform Console(Test out Terraform interpolations) </p>
        <p> echo 'join(',',
    [
      'foo',
      'bar'
    ])' | terraform console echo an expression into terraform console and see its expected result as output  </p>  
        <p> echo '1 + 5' | terraform console Terraform console also has an interactive CLI just enter “terraform console </p> 
        <p>  echo 'aws_instance.my_ec2.public_ip' | terraform console display the Public IP against the “my_ec2” Terraform resource as seen in the Terraform state file  </p>"
  },
  {
    "commands": " Terraform Graph(Dependency Graphing)",
    "text": " <p> terraform graph | dot -Tpng > graph.png produce a PNG diagrams showing relationship and dependencies between Terraform resource in your configuration/code </p> 
        <p>   Terraform Taint/Untaint(mark/unmark resource for recreation -> delete and then recreate)   </p> 
        <p> terraform taint aws_instance.my_ec2 taints resource to be recreated on next apply  </p> 
        <p> terraform untaint aws_instance.my_ec2 Remove taint from a resource </p> 
         <p> terraform force-unlock LOCK_ID forcefully unlock a locked state file, LOCK_ID provided when locking the State file beforehand  </p>"
  },
  {
    "commands": " Terraform Cloud",
    "text": " <p> terraform login obtain and save API token for Terraform cloud </p> 
        <p> terraform logout Log out of Terraform Cloud, defaults to hostname app.terraform.io </p> "
  },

    {
    "commands": " You have a critical application running in your Kubernetes cluster, and one of the Pods keeps crashing. How would you troubleshoot and resolve this issue?   ",
    "text": " <p> To troubleshoot this issue, I would follow these steps: </p>
        <p> Check the Pod's logs using kubectl logs to understand the error.</p> 
        <p>     Describe the Pod and its associated resources using kubectl describe pod <pod-name>. </p>
        <p> Verify resource limits and requests in the Pod's configuration.</p> 
        <p> Ensure that the Docker image is healthy and that there are no issues with the container itself. </p>
        <p> Inspect resource utilization on the node where the Pod is scheduled. </p>
        <p> Review the readiness and liveness probes in the Pod's configuration. </p> 
        <p> Check for network or security policy issues that may be impacting the Pod. </p>
        <p> Once the issue is identified, apply the necessary fixes and monitor the Pod's stability.  </p>"
  },
  { 
    "commands": "Your team wants to implement a Canary deployment strategy for a new microservice. How would you set up Canary releases in Kubernetes using Helm?",
    "text": "<p>To set up Canary releases in Kubernetes using Helm, I would follow these steps: </p>
        <p> Create two different Helm values files: one for the Canary release and one for the main release. </p>
        <p> Define the number of Pods for each release, applying different weights (e.g.,
    90% to the main release,
    10% to the Canary).  </p>
        <p> Configure your Ingress controller to split traffic based on headers (e.g., user agent, cookie, or custom headers).  </p> 
        <p> Deploy the main release with its values file.</p>
        <p> Deploy the Canary release with its values file. </p>
        <p> Monitor the performance and health of both releases.</p>
        <p> Gradually increase the Canary traffic share while monitoring application metrics.</p>
        <p> If the Canary release performs well, you can increase its share further until it reaches 100%.</p>"
  },
  {
    "commands": " You need to ensure high availability for the control plane of your Kubernetes cluster. What strategies and components can you implement to achieve this? ",
    "text": "<p> To ensure high availability for the control plane, I would consider the following strategies and components: </p>
        <p> Distributed Control Plane: Deploy multiple control plane nodes (e.g., master nodes) to distribute the control plane components across nodes </p>
        <p> Load Balancer: Use a load balancer to distribute traffic across the control plane nodes to ensure redundancy and fault tolerance.</p>
        <p>   etcd Clustering: Implement etcd clustering to ensure data store redundancy and resilience.</p>
        <p> Node Redundancy: Use multiple worker nodes to ensure application Pods can continue to run in the event of a node failure. </p>
        <p>Node Redundancy: Use multiple worker nodes to ensure application Pods can continue to run in the event of a node failure.  </p>
        <p> Node Autorecovery: Configure mechanisms like NodeAffinity and PodDisruptionBudgets to ensure node recovery after a failure.  </p>
        <p>Backup and Disaster Recovery: Regularly back up the etcd data store and have a disaster recovery plan in place.  </p>"
  },
  {
    "commands": "Your team is deploying a stateful application in Kubernetes, and you need to ensure that Pods maintain stable network identities and storage. How would you achieve this using Kubernetes resources? ",
    "text": "<p> To ensure that Pods maintain stable network identities and storage in Kubernetes for a stateful application, I would use the following resources:  </p>
         <p> StatefulSet: Deploy the stateful application using a StatefulSet resource. StatefulSets ensure that Pods are created and scaled in a predictable and ordered manner, maintaining stable network identities. </p>
         <p>Persistent Volume Claims (PVCs): Define PVCs for the StatefulSet to request and consume persistent storage. PVCs ensure that each Pod has stable storage volumes.  </p>
         <p>Headless Service: Use a headless service to enable network identity for each Pod. The service does not create a load balancer, and each Pod can be directly addressed using its hostname. </p>
         <p> Init Containers: Use init containers for tasks like database schema migrations that need to be performed before the main application container starts.</p>
         <p> Stateful Application Logic: Write the application code to handle stateful behaviors and maintain data integrity across Pod restarts. </p>  " 
  },
  {
    "commands": " Your team needs to deploy a highly secure Kubernetes cluster. What security best practices and mechanisms would you implement to secure the cluster?",
    "text": "<p>To secure a Kubernetes cluster, I would implement several security best practices and mechanisms, including:</p>
        <p> RBAC (Role-Based Access Control): Define fine-grained access controls to limit who can perform specific actions within the cluster.</p>
        <p> Network Policies: Use network policies to restrict communication between Pods, only allowing necessary traffic.  </p>
        <p> Pod Security Policies: Enforce security policies for Pods, specifying allowed capabilities, volumes, host namespaces, and more.  </p> 
        <p> Image Scanning: Implement container image scanning to detect and remediate vulnerabilities in container images.</p>
        <p> Secrets Management: Safely store and manage sensitive information such as API keys, passwords, and certificates using Kubernetes Secrets. </p>
        <p> Pod Hardening: Configure security context for Pods, including setting non-root users and read-only file systems. </p>
        <p> API Server Authentication: Enable API server authentication mechanisms such as client certificates, service accounts, and OIDC. </p>
        <p> API Server Authentication: Enable API server authentication mechanisms such as client certificates, service accounts, and OIDC. </p>
        <p> etcd Encryption: Encrypt etcd data at rest to protect cluster state. </p>
        <p> Regular Auditing: Set up cluster auditing to monitor and log cluster activities for security analysis and compliance.  </p>
        <p> Backup and Disaster Recovery: Regularly back up cluster configuration, etcd data, and manifests to ensure quick recovery from disasters.</p>
        "
  },
  {
    "commands": "how to check logs for a failed nodes in kubernetes?",
    "text": "<p>@@first will Kubelet Logs, and Node System Logs</p>
        <p>Kubelet Logs: The kubelet is the primary node agent responsible for managing containers and interacting with the control plane. You can check kubelet logs to identify issues with container runtime, pod scheduling, and other node-related problems.</p>
        <p> journalctl -u kubelet </p>
        <p> Node System Logs: These logs include system-level logs, such as messages from the operating system, which can provide valuable information about hardware or system-level issues on the node.</p>
        <p> journalctl -xe</p>
        <p>To check control plane logs, we can use kubectl to access the Kubernetes API server logs or other control plane component logs.  </p>
        <p>---kubectl logs -n kube-system <kube-apiserver-pod-name> kube-apiserver.</p>"
  },
  {
    "commands": "what if my kubernetes node continously getting crashed what i have to do to solve this issue?",
    "text": "If my Kubernetes node is continuously crashing, it's a critical issue that needs check stability and availability of that. Actually crashes can result from various factors, including hardware problems, resource constraints or misconfigurations.
        <p>Check Node Status: Verify the current status of the node by running kubectl get nodes to see if it's in a NotReady state.</p>
        <p> Review Node Logs: Examine the node's logs, which include kubelet, container runtime (e.g., Docker), and system logs. This can help you identify the specific issues that caused the node to crash. Use the commands mentioned in the previous response to access these logs.</p>
        <p>Hardware Check: Physically inspect the node's hardware components, including CPU, memory, disks, and network interfaces. Ensure there are no hardware failures, loose connections, or overheating issues.</p>
        <p> Resource Constraints: Analyze the resource utilization on the node. Check if the node is consistently running out of CPU, memory, or storage resources. You may need to adjust resource requests and limits for running Pods or add more nodes to your cluster.</p>
        <p> Kernel Panics:  If you observe kernel panics in the logs, it indicates a problem at the operating system level. Identify the root cause of the kernel panic, which could be due to incompatible drivers, kernel modules, or hardware issues.</p>
        <p> Container Runtime Issues:  If you are using Docker or another container runtime, need to review container runtime logs for issues related to container stability or resource limitations. Update the container runtime if necessary.</p>
        <p> Misconfigurations:  Check for misconfigurations in your Kubernetes setup. like incorrect kubelet configuration, networking issues, or cluster-wide misconfigurations that affect the node's health.</p>
        <p> Resource Overallocation: Ensure that no excessive resource overallocation is occurring on the node. Pods with resource requests and limits that exceed available resources can lead to instability.</p>
        <p> Cluster Autoscaling:  If you're using a cloud-managed Kubernetes service or cluster autoscaling, verify that autoscaling is correctly configured. This can help automatically replace failing nodes.</p>
        <p> Node Replacements: If the node continues to crash despite troubleshooting and remediation efforts, you may need to replace the node with a new one. Kubernetes will automatically schedule Pods to healthy nodes.</p>
        <p>Monitoring and Alerts:  Implement comprehensive monitoring and alerting for your cluster to detect issues early and take proactive measures. Tools like Prometheus, Grafana, and alert managers can help in this regard.</p> 
        <p> Backup and Recovery: Ensure that you have a robust backup and recovery strategy in place for your cluster's configuration and etcd data store. This is important in case you need to restore or rebuild the cluster.</p>"
  },
  {
    "commands": "A Pod running a critical service in your EKS cluster has suddenly failed. How would you troubleshoot and resolve this issue?",
    "text": "<p> Start by checking the status and logs of the Pod using kubectl describe pod and kubectl logs commands respectively.</p>
        <p> kubectl top pod <pod-name>:usage of the Pod, including CPU and memory. Resource constraints might lead to service failures.  </p>
        <p> kubectl exec -it <pod-name> -- /bin/bash  -If necessary, use kubectl exec to access the Pod's shell and investigate the environment or execute commands within the container.</p>
        <p> kubectl get pv,pvc:- If the service uses persistent storage, ensure that the associated volumes are available and accessible.</p>
        <p> * If the issue occurred after a recent deployment, consider rolling back to a previous stable version. :-kubectl rollout undo deployment <deployment-name></p>
        <p> To store and monitor our EKS cluster logs, consider using the CloudWatch Logs feature of AWS.So need to Review AWS CloudWatch logs for the EKS cluster to identify any issues at the cluster level.</p>"
  },
  {
    "commands": "You have a set of Pods running an application in EKS. How would you expose this application to outside traffic?",
    "text": "AWS Load Balancer Controller in EKS allows for exposure of services to outside traffic. By adding annotations to your service, you can create an AWS Application Load Balancer or a Network Load Balancer."
  },
  {
    "commands": "Your EKS application is experiencing higher than expected traffic. How would you automatically scale the Pods?  ",
    "text": "Set up the Kubernetes Horizontal Pod Autoscaler in EKS. For metrics needed for autoscaling, EKS supports the Kubernetes Metrics Server. You can also consider AWS Cluster Autoscaler on EKS for node-level scaling in combination with the Kubernetes Cluster Autoscaler."
  },
  {
    "commands": "Your EKS application needs to access sensitive information such as database passwords. How would you securely manage this information?",
    "text": "Kubernetes Secrets in EKS can be used for storing sensitive information. AWS Secrets Manager also provides a secure way to store secrets. AWS Secrets Manager Injector enables secrets to be directly accessed from Secrets Manager to your Kubernetes Secrets."
  },
  {
    "commands": "Your EKS application needs to write and read data from a persistent storage. How can you achieve this in EKS?",
    "text": "The Amazon EBS CSI driver in EKS can provision Amazon EBS volumes for Pods. Persistent Volumes and Persistent Volume Claims can be defined to bind Pods to EBS volumes.        "
  },
  {
    "commands": "How can you ensure separation of resources in a multi-tenant EKS cluster?",
    "text": "Use Kubernetes Namespaces for virtual separation within the EKS cluster. AWS also provides the IAM roles for service accounts feature to assign IAM permissions to Pods."
  },
  {
    "commands": "You have deployed an application in EKS using a Deployment object. You now need to update the application with zero downtime. How would you achieve this?",
    "text": "Kubernetes Deployments in EKS support Rolling updates. This strategy allows for incremental updates of Pods instances with new ones, ensuring zero downtime. "
  },
  {
    "commands": "You rolled out an update to your application in EKS, but there are unexpected errors. How would you rollback the update?",
    "text": "Use the kubectl rollout undo deployment command to rollback a Deployment in EKS. This reverts the Deployment to its previous state."
  },
  {
    "commands": "An EC2 instance serving as a Node in your EKS cluster has failed. How does EKS handle this situation?",
    "text": "The Kubernetes Node Controller in EKS marks the Node as ‘unavailable’ and reschedules the Pods to other Nodes in the EKS cluster. If your nodes are part of an EC2 Auto Scaling group, a new instance will replace the failed one."
  },
  {
    "commands": "How does EKS distribute network traffic to Pods?",
    "text": " EKS uses Kubernetes Services to define a set of Pods and route network traffic to them. Additionally, the AWS Load Balancer Controller can create an AWS Application Load Balancer or a Network Load Balancer to distribute traffic among Pods.  "
  },
  {
    "commands": "Your Prometheus instance is unable to handle the load of your growing EKS cluster. How would you scale it?",
    "text": "Scale Prometheus in EKS by dividing the targets that Prometheus scrapes into different Prometheus servers, a process known as sharding. Alternatively, you can use Thanos or Cortex for horizontal scalability of Prometheus."
  },
  {
    "commands": "You deployed an application update using Argo CD in your EKS cluster, but the new version is causing errors. How would you rollback to the previous version?",
    "text": "In the Argo CD dashboard, select the application, then click on the ‘App Details’ tab, and finally on the ‘History’ tab. Select the desired state and hit the ‘Sync’ button to rollback.   "
  },
  {
    "commands": "You need to collect and analyze metrics of the EKS control plane itself. How can you achieve this?",
    "text": "Prometheus can collect metrics from the EKS control plane. Set up service monitors for the EKS API server and other components. Use Grafana to visualize these metrics."
  },
  {
    "commands": "Your monitoring needs to be highly available, and you can’t afford to lose metrics data. How would you ensure the high availability of Prometheus?",
    "text": " Run two or more identically configured Prometheus instances in parallel for high availability. Both instances scrape the same targets, thus they have the same data. If one instance fails, the other instance provides access to your metrics data"
  },
  {
    "commands": "Have you written Yaml file to deploy k8s cluster and explain that yaml file ",
    "text": "<p>Certainly! YAML (YAML Ain't Markup Language) is commonly used for writing configuration files, and it is a popular choice for defining Kubernetes manifests. Below is a simple example of a YAML file that can be used to deploy a basic Kubernetes cluster. This example assumes you are using a tool like kubeadm to set up the cluster. </p>
        <p>yaml
        Copy code
        apiVersion: kubeadm.k8s.io/v1beta3
        kind: ClusterConfiguration
        kubernetesVersion: '1.21.3'
        controlPlaneEndpoint: 'k8s-cluster.local:6443'
        networking:
          podSubnet: '10.244.0.0/16'
        apiServer:
          extraArgs:
            apiserver-count: '3'
            runtime-config: 'api/all=true'
            service-node-port-range: '30000-32767'
        controllerManager:
          extraArgs:
            allocate-node-cidrs: 'true'
        Let's break down the key components of this YAML file:
        </p>
        <p> apiVersion and kind: These fields indicate the API version and the kind of the Kubernetes object. In this case, it's a ClusterConfiguration object.</p>
        <p>kubernetesVersion: Specifies the desired version of Kubernetes.</p>
        <p>controlPlaneEndpoint: The API server's endpoint. In this example, it's set to 'k8s-cluster.local:6443'.</p>
        <p> networking: Configures the networking for the cluster. In this example, it specifies the pod subnet as '10.244.0.0/16'.</p>
        <p> apiServer: Extra arguments for the API server, such as the number of replicas (apiserver-count), runtime configuration, and the node port range for services.</p>
        <p>controllerManager: Extra arguments for the controller manager, including the allocation of node CIDRs.</p>
        "
  },
  {
    "commands": "Explain the technical process of a app deployment in kubernetes?",
    "text": "The technical process of deploying an application in Kubernetes involves several steps, from creating containerized application images to managing Kubernetes resources. 
        <p> Containerize Your Application: Write a Dockerfile for each component of your application. This file specifies the dependencies, environment settings, and commands needed to build a Docker image.Build Docker images for your application components using the docker build command.</p>
        <p>Example Dockerfile:</p>
        <p> Dockerfile
        Copy code
        FROM node: 14
        WORKDIR /app
        COPY package*.json ./
        RUN npm install
        COPY . .
        CMD [
      'npm',
      'start'
    ]
        Build the Docker image:</p>
        <p>Dockerfile
        Copy code
        FROM node: 14
        WORKDIR /app
        COPY package*.json ./
        RUN npm install
        COPY . .
        CMD [
      'npm',
      'start'
    ]
        Build the Docker image:</p>
        <p>bash</p>
        <p>Copy code</p>
        <p>docker build -t your-registry/your-app-image:tag .</p>
        <p>Push Docker Images to a Container Registry:</p>
        <p>Push the Docker images to a container registry (e.g., Docker Hub, Google Container Registry, or an internal registry).</p>
        <p> Replace your-registry/your-app-image:tag with the actual image name and tag.</p>
        <p> docker push your-registry/your-app-image:tag</p>
        <p> Create Kubernetes Deployment Manifests:</p>
        <p>we need to Write YAML manifests to define the Kubernetes resources, starting with a Deployment for your application.</p>
        <p> Specify the container image reference in the Deployment manifest.</p>
        <p> Example deployment.yaml:</p>
        <p>apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: myapp
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: myapp
          template:
            metadata:
              labels:
                app: myapp
            spec:
              containers:
              - name: myapp-container
                image: your-registry/your-app-image:tag
        </p>
        <p> Use kubectl apply to deploy the application to the Kubernetes cluster.</p>
        <p>kubectl apply -f deployment.yaml Monitor Deployment Progress:</p>
        <p> Monitor the deployment progress using kubectl get deployments.</p>
        <p> kubectl get deployments  This shows the desired, current, and up-to-date replicas, indicating the status of the deployment.</p>
        <p> Expose the Application with a Service:</p>
        <p> Create a Service to expose your application internally or externally.</p>
        <p> Example service.yaml: </p>
        <p> apiVersion: v1
        kind: Service
        metadata:
          name: myapp-service
        spec:
          selector:
            app: myapp
          ports:
            - protocol: TCP
              port: 80
              targetPort: 8080
          type: LoadBalancer
        </p>
        <p> Apply the service manifest:</p>
        <p> kubectl apply -f service.yaml</p>
        <p> Access Your Application: </p>
        <p> Access your application using the IP or DNS provided by the Service.</p>
        <p> kubectl get services</p>
        <p> Look for the external IP or DNS of your service</p>
        <p> Scale Your Application (Optional): </p>
        <p> Scale the application by adjusting the number of replicas in the Deployment </p>
        <p> kubectl scale deployment myapp --replicas=5 </p>
        <p> Update Your Application (Rolling Updates):</p>
        <p> To update your application, modify the Deployment manifest or the container image version, then use kubectl apply. Kubernetes performs a rolling update. </p>
        <p> kubectl set image deployment/myapp myapp-container=new-image:tag  </p> "
  },
  {
    "commands": "  Can you explain how you have used GitLab in your previous projects and what specific tasks it helped you accomplish? ",
    "text": "<p> In my previous projects, GitLab was instrumental in version control and collaboration. It allowed me to track changes made by different team members, ensuring that everyone’s contributions were accounted for without overwriting each other’s work. This feature was particularly useful during the debugging process where we could easily revert back to a working version if necessary. </p>
        <p> GitLab also facilitated continuous integration/continuous deployment (CI/CD). By automating these processes, it increased our efficiency as we could immediately detect any issues or bugs, fix them promptly, and deploy the updated code seamlessly. </p>
        <p> Moreover, GitLab provided an avenue for project management. We utilized its issue tracking system to assign tasks, monitor progress, and manage workflows. Its built-in wiki served as a central repository for documentation, making information readily accessible to all team members. </p> "
  },
  {
    "commands": "  How would you migrate a project from another version control system to GitLab? ",
    "text": "<p> To migrate a project to GitLab, start by creating a new project in GitLab. Then, install and configure the Git client on your local machine. Clone the repository from the existing version control system using the appropriate command (e.g., ‘svn checkout’ for SVN). Navigate into the cloned directory and initialize it as a new Git repository with ‘git init’. Add all files to the new Git repo with ‘git add .’ and commit them with ‘git commit -m “Initial commit”‘. Finally, push the committed changes to the newly created GitLab project with ‘git push origin master’. </p>"
  },
  {
    "commands": " Can you detail the process of setting up a GitLab CI/CD pipeline for a new project and explain its benefits?",
    "text": "<p> GitLab CI/CD pipeline setup involves several steps. First, create a ‘.gitlab-ci.yml’ file in the project’s root directory. This YAML file defines the structure and order of pipelines and includes stages—units of work that can be executed concurrently—and jobs—which define what to do in each stage.</p> 
        <p> Next, configure GitLab Runner, which executes your jobs and sends results back to GitLab. Install it on a machine where you want jobs to run, then register it with GitLab using a registration token. </p>
        <p>Once set up, every code commit triggers an automated build, test, and deployment process if specified. The benefits are manifold: faster detection and resolution of bugs, streamlined software delivery, reduced risk of failed deployments, and increased confidence in the quality of the product being delivered.   </p>
        "
  },
  {
    "commands": " How would you handle a merge conflict in GitLab and what strategies do you use to avoid such conflicts? ",
    "text": " Merge conflicts in GitLab are resolved by identifying the conflicting files, opening them and manually resolving discrepancies. The changes are then committed and pushed to the repository.
        <p>
            <p> To avoid merge conflicts, I employ several strategies: </p>
            <ul>
               <li>Regular Pulls: By frequently pulling from the main branch, I ensure my local copy is up-to-date, reducing potential conflicts.</li>
               <li> Small Changes: Making small, incremental changes reduces the likelihood of conflicts.</li>
               <li> Communication: Coordinating with team members about who is working on what file helps prevent simultaneous modifications.</li>
               <li> Branching Strategy: Using feature branches for each task ensures isolation of changes until they’re ready to be merged.</li>
               <li> Code Review: Before merging, code reviews help identify potential issues that could lead to conflicts.</li>
                
            </ul>
        </p> "
  },
  {
    "commands": " Can you discuss the differences between Git and GitLab and why you might choose to use GitLab?",
    "text": " <p> Git and GitLab are both version control systems but serve different purposes. Git is a distributed revision control system, allowing multiple developers to work on a project without overwriting each other’s changes. It tracks modifications, enabling users to revert or combine changes.</p>
        <p> On the other hand, GitLab is a web-based DevOps lifecycle tool that provides a Git-repository manager. It offers features like issue tracking, continuous integration/continuous deployment (CI/CD), and more, all in one place.</p>
        <p>Choosing GitLab could be due to its additional features not present in Git. Its built-in CI/CD eliminates the need for an external source, simplifying the development process. The integrated issue tracking helps manage bugs and feature requests efficiently. Moreover, it supports team collaboration with access controls, code reviews, activity feeds, and more.</p>
        "
  },
  {
    "commands": " Explain the process of using GitLab for code review. What are the main advantages of this? ",
    "text": " <p> GitLab’s code review process begins with a developer creating a merge request (MR) after committing changes to their branch. The MR contains details of the changes, facilitating peer review. Reviewers can comment directly on lines of code or overall changes, fostering discussion and collaboratio</p>
        <p> The advantages of GitLab for code review include its built-in continuous integration/continuous deployment (CI/CD), which automatically tests code before merging, reducing errors. It also provides an audit trail of changes and discussions, enhancing accountability. Furthermore, it encourages team collaboration and knowledge sharing, improving code quality and consistency.</p> 
        "
  },
  {
    "commands": "  Can you detail an instance where you had to optimize a GitLab CI/CD pipeline for better performance? What steps did you take?",
    "text": "<p> In a previous project, I optimized a GitLab CI/CD pipeline to enhance performance. The initial issue was the long build times due to unnecessary jobs running for every code push.</p>
        <p> To address this, I implemented selective job execution using ‘only’ and ‘except’ keywords in .gitlab-ci.yml file. This allowed certain jobs to run only when specific files or branches were modified, reducing redundant builds.</p> 
        <p> Next, I utilized parallel testing by splitting tests into multiple smaller jobs that could run concurrently. This significantly reduced the overall test time. </p> 
       <p> I also introduced caching of dependencies. By storing them between pipelines, we avoided re-downloading and rebuilding on each run, saving considerable time. </p>
       <p> Lastly, I used Docker images for quicker setup times. Instead of installing all required tools for each job, they were pre-installed in the Docker image, which was then reused across jobs.</p> 
      <p> These steps led to a significant reduction in our pipeline’s runtime, improving efficiency and productivity. </p>
       "
  },
  {
    "commands": " How do you manage and implement access control for different users in GitLab?  ",
    "text": " <p>In GitLab, access control is managed through a role-based permission model. Users are assigned roles that define their permissions within the system. There are five predefined roles: Guest, Reporter, Developer, Maintainer, and Owner. Each role has increasing levels of access to project features. </p>
        <p> To implement access control, an administrator assigns users to these roles at either the group or project level. At the group level, permissions cascade down to all projects within the group. At the project level, permissions apply only to that specific project.</p>
       <p> For more granular control, Protected Branches can be used. They restrict who can push to or merge into a branch. This is defined by the user’s role or by named users. Similarly, Deploy Keys allow read-only or read-write access to a single project.</p>  
       "
  },
  {
    "commands": " Discuss the importance of GitLab webhooks and provide an example of when you’ve used them.  ",
    "text": "  <p> GitLab webhooks are crucial for automating tasks in the software development process. They allow external services to be notified when certain events occur within a repository, such as push events or merge requests. This automation enhances efficiency and consistency. </p>
         <p>In my experience, I’ve used GitLab webhooks to integrate with Jenkins, an open-source automation server. When code was pushed to the repository, a webhook triggered a build on Jenkins. This ensured that any changes were immediately tested, providing rapid feedback to developers about potential issues.  </p>
        "
  },
  {
    "commands": "Can you provide a scenario where you might use GitLab’s integration with Docker for your project deployment?",
    "text": " <p> GitLab’s Docker integration can be beneficial in a scenario where I’m developing a web application with a team. We’d use GitLab for version control and collaboration, while Docker would provide an isolated environment to build and test our app.</p> 
        <p> Once a feature is ready, it’s committed to GitLab. The CI/CD pipeline automatically triggers, building a new Docker image from the updated codebase. This image is then pushed to a Docker registry within GitLab.</p>
       <p> The testing phase follows, where automated tests run against the newly built Docker image. If tests pass, the image is deployed to the staging environment for further validation. Upon successful validation, the Docker image is finally deployed to the production environment.</p>
       <p> This process ensures seamless deployment, reduces “it works on my machine” issues, and enhances productivity by allowing developers to focus more on coding rather than troubleshooting environmental discrepancies.</p>
       "
  },
  {
    "commands": " How have you used GitLab’s issue tracking feature in your projects? ",
    "text": "<p> In my projects, GitLab’s issue tracking feature has been instrumental in managing tasks and bugs. I’ve used it to create issues that represent tasks, which are then assigned to team members. Each issue includes a description of the task, labels for categorization, and a milestone indicating its deadline.</p>
         <p> I also use the weight feature to estimate the effort required for an issue, aiding in project planning. The time tracking feature helps monitor how much time was actually spent on the task.</p>
        <p> For bug tracking, I follow a similar process but include steps to reproduce the bug in the issue description. Once resolved, the issue is closed with a comment linking to the merge request that contains the fix. </p>"
  },
  {
    "commands": " Can you explain the procedure to set up and automate tests in GitLab? ",
    "text": " GitLab’s Continuous Integration (CI) feature automates testing. To set it up, create a .gitlab-ci.yml file in your project root directory. This YAML file defines the structure and order of pipelines and includes jobs that make up steps within each pipeline stage. 
         <p> To automate tests, define them as jobs within stages. For example: </p>
        <p> stages:
        - test </br>
      job1:
        stage: test
        script: 'execute-script-for-testing'</br>
      In this case, ‘test’ is the stage name, ‘job1’ is the job name, and ‘script’ executes the test. GitLab CI/CD runs these jobs using runners, which are isolated (virtual) machines that pick up jobs through the coordinator API of GitLab CI/CD.</br>
      
      Once you push the .gitlab-ci.yml to GitLab, every commit or push triggers the CI pipeline with defined jobs. You can view the status, logs, and results on the GitLab interface under “CI/CD > Pipelines”.
      </p>
        "
  },
  {
    "commands": " What experience do you have with GitLab’s APIs and how have you used them in your projects? ",
    "text": " <p> I have extensive experience with GitLab’s APIs, particularly in automating tasks and integrating third-party services. I’ve used the REST API for creating and managing projects, users, issues, and merge requests programmatically. For instance, in one project, I automated the process of issue creation based on certain triggers using Python scripts that interacted with the API. In another, I integrated a CI/CD pipeline with GitLab’s API to automatically build, test, and deploy code changes. Additionally, I’ve utilized Webhooks to trigger actions upon specific events like push or merge request. </p> "
  },
  {
    "commands": " How do you typically handle branch management in GitLab for a large project? ",
    "text": " <p> In GitLab, branch management for a large project typically involves creating feature branches from the main branch. Each new feature or bug fix is developed in its own branch to avoid conflicts and maintain code integrity. Merge requests are used to integrate changes back into the main branch after peer review. Branches are deleted post-merge to keep the repository clean. For release management, we use protected branches that only select members can merge into. This ensures stability of production-ready code.</p> "
  },
  {
    "commands": " Describe a time when you had to troubleshoot a complex problem with GitLab. How did you go about it? ",
    "text": " <p> In a previous project, we faced an issue where GitLab CI/CD pipelines were failing intermittently. The error logs indicated a problem with the Docker executor. To troubleshoot, I first isolated the problem to ensure it was not related to code changes by running the pipeline on previously successful commits. When the error persisted, I focused on the GitLab Runner configuration.</p> 
        <p> I checked the runner’s version and found it outdated. After updating it, the problem still existed. Then, I examined the Docker daemon logs for any anomalies. I noticed that the system ran out of memory during peak usage times. This led me to believe that the Docker executor might be consuming more resources than allocated.</p> 
        <p> To confirm this, I monitored resource usage while triggering multiple pipelines simultaneously. As suspected, the Docker executor was indeed exceeding its resource limit causing the pipelines to fail. I resolved the issue by increasing the resource allocation for the Docker executor in the GitLab Runner configuration file. Post-adjustment, the pipelines executed successfully without any intermittent failures. </p>"
  },
  {
    "commands": " How would you configure GitLab runners for a project and why are they important? ",
    "text": " <p> GitLab runners are crucial for automating tasks in software development. They execute defined jobs and send results back to GitLab, enabling continuous integration (CI) or continuous deployment (CD). </p> 
         <p> To configure a runner, first install it on a server that can access your GitLab instance. Then register the runner using the registration token from your project’s settings page. Specify the executor during registration; this determines how the runner will handle jobs. Executors include shell, docker, virtualbox, etc., each with unique advantages.</p>
        <p>  After registration, edit the ‘config.toml’ file to fine-tune runner behavior. This includes concurrent job limits, custom environment variables, and Docker image specifications if using Docker executor.</p>
        <p> Runners are important as they automate testing and deployment, reducing human error and speeding up development cycles. They also allow parallel execution of jobs, improving efficiency.</p>
        "
  },
  {
    "commands": " Discuss how GitLab’s environment-specific configurations can be beneficial in project deployment. ",
    "text": " <p> GitLab’s environment-specific configurations offer several benefits in project deployment. They allow for the creation of distinct environments like development, testing, and production, each with unique settings. This segregation aids in maintaining code quality as changes can be tested in isolated environments before being pushed to production </p> 
         <p> Moreover, these configurations support automated deployments. With GitLab CI/CD pipelines, code modifications are automatically built, tested, and deployed to specified environments based on predefined rules. This automation reduces human error and accelerates delivery times.</p>
        <p> Additionally, they facilitate rollbacks. If a deployment fails or causes issues, it is possible to revert back to a previous stable state quickly. This feature enhances system reliability and availability.</p>
        <p>Furthermore, GitLab provides detailed monitoring and logging for each environment. These insights help in identifying performance bottlenecks, debugging issues, and making informed decisions about resource allocation.</p>"
  },
  {
    "commands": "  How do you use the protected branches and tags feature in GitLab",
    "text": " <p> In GitLab, protected branches and tags prevent unauthorized modifications. To use this feature, navigate to your project’s settings, then select ‘Repository’. Under ‘Protected Branches’ or ‘Protected Tags’, you can specify which branches/tags are protected. </p>
        <p> For branches, choose the branch name from the dropdown menu, then set the access levels for “Allowed to push” and “Allowed to merge”. For tags, simply define a wildcard or specific tag name and assign who is allowed to create them.</p>
       <p> You can also utilize code owners in conjunction with protected branches. Code owners are defined in a CODEOWNERS file in the repository root or .gitlab directory. When changes affect owned files, those users must approve the merge request.</p>
        "
  },
  {
    "commands": " Can you explain GitLab’s squash and merge feature and why it’s useful? ",
    "text": " <p> GitLab’s squash and merge feature is a version control functionality that combines multiple commits into one, before merging it with the main codebase. This feature is useful for maintaining a clean and understandable commit history.</p>
        <p> When working on a feature or bug fix, developers often make numerous small commits to save their progress. While this practice is beneficial during development, it can clutter the project’s commit history when merged without squashing. A cluttered history makes it difficult to understand the evolution of the project and identify the purpose of each change.  </p>
       <p> By using the squash and merge feature, all changes made in a branch are combined into a single commit with a new message describing the overall modification. This results in a linear and comprehensible commit history where each commit represents a significant change or addition to the codebase.</p>
       <p> Moreover, squashing commits also simplifies the process of reverting changes. If an issue arises from a merged feature, instead of identifying and reverting individual commits, the entire feature can be removed by reverting the single squashed commit.</p>
        "
  },
  {
    "commands": " How have you used GitLab’s artifact management features in your projects? ",
    "text": " <p> In my projects, I’ve utilized GitLab’s artifact management features to streamline development and deployment processes. Specifically, I’ve used job artifacts for temporary storage of generated files after a pipeline job execution. This has been particularly useful in storing test results or binaries that are needed for later stages.</p>
        <p> I’ve also leveraged the ‘artifacts:expire_in’ feature to automatically remove older, unnecessary artifacts, optimizing storage usage. Additionally, I’ve employed dependencies to pass artifacts between different jobs within the same pipeline, enhancing efficiency.</p>
        <p> Furthermore, I’ve made use of the ‘artifacts:browse’ feature to navigate through directories of artifacts directly from the GitLab interface, which has significantly improved accessibility and ease of use. Lastly, by using ‘artifacts:reports’, I was able to generate downloadable reports, aiding in project documentation and review.</p>
        "
  },
  {
    "commands": "Can you discuss GitLab’s container registry and when it would be beneficial to use it?",
    "text": " <p> GitLab’s container registry is a built-in Docker Registry, allowing users to build, push and share Docker images within GitLab. It provides an end-to-end workflow for binary packaging, eliminating the need for external services or third-party applications.</p>
        <p>The primary benefit of using GitLab’s container registry lies in its seamless integration with existing GitLab features. For instance, it works well with GitLab CI/CD, enabling automated building, testing, and deployment of Docker images. This automation reduces manual intervention, thus increasing efficiency and reducing errors.</p>
        <p>Moreover, it offers security benefits. The images are stored securely on GitLab servers, and access control can be managed directly through GitLab permissions. This ensures that only authorized individuals have access to your Docker images.</p>
        <p>In scenarios where teams frequently use Docker containers, especially in DevOps environments, GitLab’s container registry proves beneficial. It simplifies image management, enhances collaboration, and improves overall project workflows.</p>

        "
  },
  {
    "commands": " How would you set up and monitor a project’s performance in GitLab?",
    "text": " <p> To set up a project in GitLab, create a new project from the dashboard by clicking on “New Project”, fill out necessary details and click “Create Project”. To monitor performance, use GitLab’s built-in Continuous Integration (CI) / Continuous Deployment (CD) pipelines. These allow you to automate tests and deployments of your code. You can view pipeline status directly on the project page.</p>
        <p>For more detailed monitoring, integrate Prometheus with GitLab. It provides valuable insights into operations running within the CI/CD pipelines. Navigate to “Settings” > “Integrations” > “Prometheus”. Click “Add Prometheus service”, then configure it as per requirements.</p>
        <p>GitLab also offers Cycle Analytics that measures how long it takes for an idea to go from creation to being deployed. Access this via “Analytics” > “Cycle Analytics”.</p>
        "
  },
  {
    "commands": "Can you explain how you would use GitLab’s static and dynamic application security testing features?",
    "text": "<p>GitLab’s Static Application Security Testing (SAST) is used to analyze source code for potential vulnerabilities. To use it, you need to include the SAST GitLab CI/CD configuration in your project’s .gitlab-ci.yml file and run a pipeline.</p>
        <p> Dynamic Application Security Testing (DAST), on the other hand, analyzes running applications for vulnerabilities. It requires an environment where your application can be deployed and accessed by DAST. You add the DAST GitLab CI/CD configuration to your .gitlab-ci.yml file, ensure your app is accessible via a URL, and then run a pipeline.</p>
        <p> Both SAST and DAST provide feedback through merge request widgets and security dashboards, allowing easy identification of issues that need addressing.</p>
        "
  },
  {
    "commands": "Discuss your experience with integrating third-party tools into GitLab. Can you give some examples?",
    "text": "<p> In my experience, integrating third-party tools into GitLab has been straightforward due to its robust API and webhooks. I’ve integrated project management tools like Jira for issue tracking, which involved configuring the Jira service in GitLab and setting up a webhook in Jira. This allowed seamless synchronization of issues between both platforms.</p>
        <p>Another integration was with Jenkins for continuous integration/continuous deployment (CI/CD). By using GitLab’s CI/CD configuration file (.gitlab-ci.yml), I could trigger Jenkins builds upon code commits.</p>
        <p> I also integrated SonarQube for static code analysis. After installing the SonarScanner CLI in our pipeline and configuring the sonar-project.properties file, we were able to analyze code quality on each merge request</p>
        <p> Lastly, I integrated Docker for containerization. Using GitLab’s registry, I built Docker images directly from source code stored in GitLab repositories, simplifying our deployment process.</p>
        "
  },
  {
    "commands": " How do you manage and keep track of GitLab updates to ensure your team’s workflow isn’t disrupted by new features or changes?",
    "text": "<p>To manage GitLab updates, it’s crucial to subscribe to the GitLab blog and follow their Twitter account for real-time update notifications. Regularly checking the ‘Upcoming Releases’ page on GitLab’s website provides insight into planned changes. For each new release, thoroughly review the Release Notes detailing new features, improvements, and bug fixes. This allows you to anticipate potential disruptions and plan accordingly. It’s also beneficial to have a staging environment mirroring your production setup where you can test new releases before deploying them in your live environment.</p>"
  },
  {
    "commands": " how to run a gitlab pipeline that integrated with docker?",
    "text": "<p>ChatGPT</p> 
        <p> Running a GitLab pipeline integrated with Docker involves configuring a .gitlab-ci.yml file in your GitLab repository to define the stages, jobs, and scripts needed to build and execute your Dockerized application. Here's a basic example to get you started:</p>
        <p> Assuming you have a Dockerfile in your project's root directory, here's a simple .gitlab-ci.yml file:</p>
        <p>
        yaml
        Copy code
        stages:
          - build
          - test
        
        variables:
          DOCKER_IMAGE: 'your-docker-image-name:latest' </br>
        
        before_script:
          - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY </br>
        
        build:
          stage: build
          script:
            - docker build -t $DOCKER_IMAGE . </br>
        
        test:
          stage: test
          script:
            - docker run $DOCKER_IMAGE your-test-command 
        </p>
        <p> stages Section</p>
        <p> Defines the different stages in your CI/CD pipeline. In this example, there are two stages: build and test. variables Section:</p>
        <p> Defines variables that can be used throughout the pipeline. Here, DOCKER_IMAGE is used to set the name of the Docker image. before_script Section:</p>
        <p> Contains commands that will be run before each job. In this case, it logs in to the GitLab Container Registry using the provided CI variables.build Job:</p>
       <p> Creates a Docker image from the Dockerfile in the repository and tags it with the specified image name. test Job: </p>
       <p> Runs tests within a Docker container based on the built image. Adjust the your-test-command placeholder with the actual command to run your tests.</p>
       <p> Before using this configuration, make sure to set the following CI/CD variables in your GitLab project: </p>
        <p> CI_REGISTRY_USER: Your GitLab username.</br>
        CI_REGISTRY_PASSWORD: Your GitLab personal access token or CI job token. This token should have the necessary permissions to pull and push to the GitLab Container Registry.</br>
        Additionally, ensure that your GitLab Runner has Docker installed and is properly configured to run Docker commands.
        </p>
       "
  },
  {
    "commands": "  what are gitlab plugins we need to install with maven,sonarqube,nexus and deployment to tomcat?",
    "text": "<p>GitLab itself does not have plugins for Maven, SonarQube, Nexus, or Tomcat. Instead, these integrations are typically handled through configuration in your GitLab CI/CD pipeline. You'll need to include the necessary commands and settings in your .gitlab-ci.yml file to achieve the desired integration. Below is a brief guide for each tool:</p>
        <p>Maven Integration:</p>
        <p> You'll need to include Maven commands in your GitLab CI script to build and package your Java project. Make sure Maven is installed on your GitLab Runner.</p>
        <p> stages:
        - build </br>
      
      build:
        stage: build </br>
        script:
          - mvn clean install</br>
      </p>
      <p> SonarQube Integration:</p>
      <p> use the SonarScanner for Maven to analyze your code and send the results to SonarQube. You'll need to configure the SonarQube server URL and authentication tokens.</p>
       <p> stages:
       - analyze   </br>
     analyze:
       stage: analyze </br>
       script:
         - mvn sonar:sonar </br>
       only:
         - master
     </p>
     <p> Nexus Integration:</p>
     <p> Deploy your Maven artifacts to Nexus by configuring the distribution management section in your pom.xml file and providing Nexus credentials in your GitLab CI/CD pipeline.</p>
       <br>
       xml
       <distributionManagement></br>
         <repository></br>
           <id>nexus</id> </br>
           <url>https: //nexus.example.com/repository/maven-releases/</url> </br>
         </repository> </br>
       </distributionManagement> </br>
       </>
       <p>
       stages:
       - deploy </br>
     
     deploy:
       stage: deploy </br>
       script:
         - mvn deploy </br>
       only:
         - master </br>
       </p>
       <p> Deployment to Tomcat:</p>
       <p> Deploying to Tomcat can be done using the Tomcat Maven Plugin. Configure the plugin in your pom.xml file and provide Tomcat server details in your GitLab CI/CD pipeline. </p>
      <br>
      xml
      <build> </br>
        <plugins> </br>
          <plugin> </br>
            <groupId>org.apache.tomcat.maven</groupId> </br>
            <artifactId>tomcat7-maven-plugin</artifactId> </br>
            <version>2.2</version> </br>
            <configuration> </br>
              <url>http: //tomcat-server:8080/manager/text</url> </br>
              <server>TomcatServer</server> </br>
              <path>/your-app</path> </br>
            </configuration> </br>
          </plugin> </br>
        </plugins> </br>
      </build> </br>
      </>
      <p> 
      stages:
        - deploy </br>
      
      deploy:
        stage: deploy </br>
        script:
          - mvn tomcat7:deploy </br>
        only:
          - master 
      </p>
      <p> Ensure that you have the necessary environment variables or GitLab CI/CD variables set for sensitive information such as Nexus and Tomcat credentials.</p>
       "
  },
  {
    "commands": " what is terraform and how to learn terraform from basics",
    "text": " <p> Terraform is an open-source infrastructure as code (IaC) tool that allows you to manage and provision your cloud infrastructure using code. It supports multiple cloud providers such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and more.</p>
        <p> Here are some steps to learn Terraform from basics:</p>
        <p> Understand the basics of infrastructure as code: Before you start learning Terraform, it's important to understand the basic concepts of infrastructure as code. This includes understanding the benefits of IaC, the different types of IaC tools available, and the best practices for using IaC. </p>
        <p> Install Terraform: Once you have a basic understanding of IaC, you can install Terraform on your local machine. You can download Terraform from the official website and follow the installation instructions for your operating system. </p>
        <p> Learn the Terraform syntax: Terraform uses its own syntax to define infrastructure as code. You can start by learning the basic Terraform syntax, such as how to define resources, variables, and data sources. </p>
        <p> Explore Terraform providers: Terraform supports multiple cloud providers, and each provider has its own set of resources and data sources. You can explore the documentation for your chosen cloud provider to understand the resources and data sources available. </p>
        <p> Create your first Terraform configuration: Once you have a basic understanding of the Terraform syntax and providers, you can create your first Terraform configuration. Start with a simple configuration that creates a single resource, such as an EC2 instance on AWS. </p>
       <p> Test and validate your configuration: After you have created your Terraform configuration, you can test and validate it. Terraform provides several tools for testing and validation, such as the 'terraform plan' command, which shows you the changes that Terraform will make to your infrastructure. </p>
       <p> Continuously improve your Terraform skills: Terraform is a powerful tool, and there is always more to learn. You can continue to improve your Terraform skills by exploring advanced topics, such as modules, remote state, and best practices for managing Terraform code. </p>
       <p> In summary, learning Terraform from basics requires understanding the basics of infrastructure as code, installing Terraform, learning the syntax, exploring providers, creating a configuration, testing and validating it, and continuously improving your skills. </p>
       "
  },
  {
    "commands": " how to define resources, variables, and data sources in terraform with examples",
    "text": " <p> Here are some examples of how to define resources, variables, and data sources in Terraform: </p> 
        <p> Resources:</p>
        <p> A resource in Terraform represents an infrastructure object, such as an EC2 instance, a virtual machine, or a database. Resources are defined using the 'resource' block in Terraform configuration. Here's an example:  </p>
        <p> code </p>
        <> resource 'aws_instance' 'example' {
            ami           = 'ami-0c55b159cbfafe1f0'
            instance_type = 't2.micro'
          
            tags = {
              Name = 'example-instance'
      }
    }
          </>
      <p> In this example, we're defining an AWS EC2 instance using the'aws_instance' resource block. We're setting the AMI ID, instance type, and tags for the instance.</p>
      <p>Variables:</p>
      <p>Variables in Terraform are used to parameterize Terraform code. They allow you to pass values into your configuration at runtime, which can make your code more flexible and reusable. Variables are defined using the 'variable' block in Terraform configuration. Here's an example: </p>
      <p>code</p>
      <p> data 'aws_s3_bucket' 'example' {
        bucket = 'example-bucket'
    } </br>
      In this example, we're defining a data source that retrieves information about an existing S3 bucket named 'example-bucket'. We can use this
      data source in our Terraform configuration to reference the bucket's ARN or other attributes. </br>
      
      These are just a few examples of how to define resources, variables, and data sources in Terraform. The syntax and usage may vary depending
      on the cloud provider and the specific resource or data source being used.</p>
   "
  },
  {
    "commands": " what are terraform files with explinations by taking example ",
    "text": " <p>Sure, here are some examples of Terraform files and their explanations: </p>
         <p> Provider Configuration File This file is used to define the provider configuration to be used by Terraform. For example, to configure AWS provider: </p>
        <p>  code</p>
        <>
        provider 'aws' {
            access_key = 'AWS_ACCESS_KEY_ID'
            secret_key = 'AWS_SECRET_ACCESS_KEY'
            region     = 'us-west-2'
    }
        </>
        <p> In this example, the provider is set to AWS, and the access key and secret key are set to the appropriate values. The region is also set to US West (Oregon).</br>

        Resource Configuration File
        This file is used to define the infrastructure resources to be created, modified, or deleted. For example, to create an EC2 instance:
        </p>
        <p>code</p>

        <>
        resource 'aws_instance' 'example' {
            ami           = 'ami-0c55b159cbfafe1f0'
            instance_type = 't2.micro'
            key_name      = 'my_key'
            security_groups = ['sg-0123456789abcdef'
      ]
    }
          
        </>
        <p>
        In this example, the resource is set to AWS EC2 instance. The ami parameter is set to the Amazon Machine Image ID of the instance. </br>
       The instance_type parameter is set to t2.micro. The key_name parameter is set to the name of the SSH key pair to be used for login to the instance. </br>
        The security_groups parameter is set to the security group ID that allows inbound traffic to the instance. </br>
        </p>
      
        <p> Variable Definition File </br>
        This file is used to define variables that can be used to customize resource configurations. For example, to define a variable for the instance type:
        <p> code</p>
        </p>

        <>
        output 'public_ip' {
            value = aws_instance.example.public_ip
    }
        </>
        
        <p>In this example, the output is set to the public IP address of the AWS EC2 instance created earlier. The value parameter is set to the public IP address of the instance. </p>
        "  
  }
    {
    "commands": " how to get aws information by using terraform",
    "text": " <p> To get AWS information using Terraform, you can use the data resource type in your Terraform configuration file. The data resource type allows
        you to retrieve information about resources that already exist in your AWS account, such as EC2 instances, security groups, VPCs, and more. </br>
        
        Here is an example of how to retrieve information about an AWS EC2 instance using the aws_instance data source:  </p>
        <p> code </p>
        <>
        data 'aws_instance' 'example' {
            instance_id = 'i-0123456789abcdefg'
    }
        </>
        <>
        output 'instance_public_ip' {
            value = data.aws_instance.example.public_ip
    }
        </>
        <p>In this example, the data block defines a new aws_instance data source named example. The instance_id attribute specifies the ID of the
        EC2 instance that you want to retrieve information about. </br>
        
        The output block defines a new output variable named instance_public_ip, which retrieves the public IP address of the EC2 instance using
        the public_ip attribute of the data.aws_instance.example object. </br>
        
        You can use other data sources provided by Terraform to get information about other AWS resources, such as aws_security_group, aws_vpc,
        and aws_subnet. You can find a list of all available data sources in the Terraform documentation.
        </p>
        "      
  }
    {
    "commands": " how to get the eks backup data using terraform",
    "text": " <p>
        Amazon Elastic Kubernetes Service (EKS) does not natively provide a backup solution. However, you can use third-party backup tools,
        such as Velero, to backup EKS clusters and resources. </br>
        
        To configure Velero with Terraform to backup your EKS cluster, follow these steps: </br>
        
        Install Velero on your EKS cluster using Terraform. You can use the helm_release resource type to install Velero as a Helm chart.   </p>
        <p> code </p>
        <>
        resource 'helm_release' 'velero' {
            name       = 'velero'
            repository = 'https: //velero.github.io/helm-charts'
            chart      = 'velero'
            version    = '2.23.7'
            
            set {
              name  = 'velero.existingSecret'
              value = 'velero-credentials'
      }
          
            set {
              name  = 'velero.storageLocation.name'
              value = 'default'
      }
          
            set {
              name  = 'velero.storageLocation.bucket'
              value = 'my-velero-bucket'
      }
          
            set {
              name  = 'velero.storageLocation.config.region'
              value = 'us-west-2'
      }
    }
        </>
        <p> In this example, we're using the helm_release resource to install the Velero chart. We're also configuring the velero.existingSecret
        to reference the secret containing AWS credentials, and setting up the storage location for backups.</br>     
        Create a Kubernetes Backup object to define the resources you want to backup. You can use the kubernetes_manifest resource type to create a Backup object.</br>
        code
        </p>
        <>
        resource 'kubernetes_manifest' 'backup' {
            yaml_body = <<EOF
          apiVersion: velero.io/v1
          kind: Backup
          metadata:
            name: my-eks-backup
            namespace: velero
          spec:
            includedNamespaces:
              - default
            excludedResources:
              - namespaces
          EOF
    }
          
        </>
        <p>In this example, we're using the kubernetes_manifest resource to create a Backup object named my-eks-backup. We're specifying that we
        want to backup all resources in the default namespace, but we're excluding the namespaces resource from the backup.</br>
        
        Apply the Terraform configuration to create the Velero installation and the Backup object. You can run terraform apply to apply the configuration.
        Once the Velero installation is complete, you can run backups of your EKS resources using the velero backup create command. </br>
        
        Note that this is just an example of how to backup EKS resources using Velero with Terraform. You should review the Velero documentation
        to configure backup and restore operations to your specific needs.
        </p>
     
        "
  },
  {
    "commands": "after storing eks data how to get that using terraform",
    "text": "<p>If you have stored EKS backup data using Velero or any other backup tool, you can use Terraform to retrieve that backup data and restore it back to your EKS cluster. </br>

        To retrieve EKS backup data using Terraform, you need to first install and configure Velero or any other backup tool on your EKS cluster as described
        in my previous answer. Once you have created a backup using Velero, you can use the velero_restore resource type in your Terraform configuration file
        to restore the backup data.</br>
        
        Here's an example of how to restore an EKS backup using Velero and Terraform:</p>
        <p> code </p>
        <>
        resource 'velero_restore' 'my-eks-restore' {
            name         = 'my-eks-restore'
            backup_name  = 'my-eks-backup'
            namespace    = 'default'
            wait_timeout = '30m'
            
            selector = {
              'app' = 'my-app'
      }
    }
        </>
        <p>In this example, we're using the velero_restore resource to restore the backup named my-eks-backup. We're specifying the namespace and selector
        for the resources that we want to restore. We're also setting the wait_timeout parameter to 30 minutes to ensure that the restore operation completes successfully.</br>
        
        Once you have created the velero_restore resource, you can run terraform apply to restore the backup data to your EKS cluster.</br>
        
        Note that the velero_restore resource type is specific to Velero. If you're using a different backup tool, you should check the documentation for
        that tool to see if there are any Terraform resources available for restoring backups.
        </p>
        "
  },
  {
    "commands": "ls -ltr",
    "text": "<p> list of all files & directories as per timestamp </p> "
  },
  {
    "commands": "mkdir d1",
    "text": "<p> create an empty directory </p>"
  },
  {
    "commands": "mkdir -p d1/d2/d3",
    "text": "<p> create a directory path </p>"
  },
  {
    "commands": "chmod 755 f1",
    "text": "<p> changing permissions for file f1 </p>"
  },
  {
    "commands": "chmod -R 755 d1",
    "text": "<p> change permissions for a directory & sub-directories </p>"
  },
  {
    "commands": "chown -R d1",
    "text": "<p> change ownership of a directory & sub-directories </p>"
  },
  {
    "commands": "mv f1 f2",
    "text": "<p> rename f1 to f1 </p>"
  },
  {
    "commands": "cp -pr",
    "text": "<p> copy a directory & sub-directories to another direcctory as per permissions & timestamp </p>"
  },
  {
    "commands": "rm -rf",
    "text": "<p> remove a file or directory even if the owner doesnot have write permissions </p>"
  },
  {
    "commands": "head -3 f1",
    "text": "<p> display first 3 lines of a file f1 </p>"
  },
  {
    "commands": "tail -3 f1",
    "text": "<p> display last 3 lines of a file f1 </p>"
  },
  {
    "commands": "tail -f f1",
    "text": "<p>continuosly monitering a file f1</p>"
  },
  {
    "commands": "find /home/ -name *.xml",
    "text": "<p> find all files with extension .xml in a directory and its sub-directories </p>"
  },
  {
    "commands": "find /home/ -size +10m",
    "text": "<p> find all the files which are more than 10mb in a directory </p>"
  },
  {
    "commands": "find /home/ -mtime +6",
    "text": "<p> find all the files which are older than 6 days </p>"
  },
  {
    "commands": "find /home/ -mtime -6",
    "text": "<p> find all the files which are modified in last 6 days </p>"
  },
  {
    "commands": "find -l /home/ -name [aA]*",
    "text": "<p> find all the files that are starting with a(uppercase or lower case) </p>"
  },
  {
    "commands": "grep KW f1",
    "text": "<p> find the lines in a file with keyword KW </p>"
  },
  {
    "commands": "grep -i KW f1",
    "text": "<p> find the lines in a file with keyword KW ignoring the case </p>"
  },
  {
    "commands": "grep -v KW f1",
    "text": "<p> find the lines without the keyword in file f1 </p>"
  },
  {
    "commands": "grep -r KW *",
    "text": "<p>find the lines with a keyword KW in files from a directory & its sub-directories</p>"
  },
  {
    "commands": "grep ^A f1",
    "text": "<p>fins the lines that start with letter A</p>"
  },
  {
    "commands": "grep Z$ f1",
    "text": "<p> fins the lines that end with letter Z</p>"
  },
  {
    "commands": "grep -v ^$ f1 > f2 ; mv f2 f1",
    "text": "<p> remove all blank lines from f1  </p>"
  },
  {
    "commands": "tar cvf d1.tar d1",
    "text": "<p>tar a file d1</p>"
  },
  {
    "commands": "tar zcvf d1.tgz d1",
    "text": "<p>bundle & compress a file d1</p>"
  },
  {
    "commands": "tar zxvf d1.tgz",
    "text": "<p>unzip & extract file d1.tgz</p>"
  },
  {
    "commands": "tar xvf d1.tar",
    "text": "<p>extract a file d1.tar</p>"
  },
  {
    "commands": "gzip",
    "text": "<p>zip a file f1</p>"
  },
  {
    "commands": "gunzip f1.zip",
    "text": "<p>unzip a file f1.zip</p>"
  },
  {
    "commands": "ps -ef",
    "text": "<p>list all the processes running in the system</p>"
  },
  {
    "commands": "kill -9 PID",
    "text": "<p>terminate a process</p>"
  },
  {
    "commands": "kill -3 PID",
    "text": "<p>take a tread dump</p>"
  },
  {
    "commands": "kill -1",
    "text": "<p>kill all the processes including the current session</p>"
  },
  {
    "commands": "top",
    "text": "<p>find cpu utilization</p>"
  },
  {
    "commands": "top -b -n 1 > top-out.txt",
    "text": "<p>capture the output of top command in top-out.txt file</p>"
  },
  {
    "commands": "free -m",
    "text": "<p>find free memory on the system</p>"
  },
  {
    "commands": "ulimit -a",
    "text": "<p>find the file limit for the user</p>"
  },
  {
    "commands": "uname -a",
    "text": "<p>find the type of operating syatem</p>"
  },
  {
    "commands": "id",
    "text": "<p>identity of the current user</p>"
  },
  {
    "commands": "cat /etc/passwd",
    "text": "<p>find all the users in the system</p>"
  },
  {
    "commands": "df -k",
    "text": "<p>find how many files are there in the system & how much space is free</p>"
  },
  {
    "commands": "du -sk *",
    "text": "<p>find the size of directories</p>"
  },
  {
    "commands": "du -sk * | sort -n",
    "text": "<p>find the size of directories & sort them in increasing order</p>"
  },
  {
    "commands": "find /home/ -name *",
    "text": "<p>find all the files in a directory & its sub-directories</p>"
  },
  {
    "commands": "ifconfig -a",
    "text": "<p>ipaddress of a system</p>"
  },
  {
    "commands": "netstat -na",
    "text": "<p>find the network statistics of a system</p>"
  },
  {
    "commands": "netstat -na | grep 8080",
    "text": "<p>find if a given port is in use or not</p>"
  },
  {
    "commands": "lsof -i tcp:9090",
    "text": "<p>find which process is using the port 9090</p>"
  },
  {
    "commands": "nslookup <IP> or nslookup <host-name>",
    "text": "<p>find website name/hostname of a given IP</p>"
  },
  {
    "commands": "traceroute <websiteurl>",
    "text": "<p>find the no. of users a request is passing through to reach a website</p>"
  },
  {
    "commands": "nc -z <IP> <PORT>",
    "text": "<p>find if a specific port on a specific IP is listening for requests or not</p>"
  },
  {
    "commands": "ping <IP>",
    "text": "<p>check the connectivity between two systems</p>"
  },
  {
    "commands": "wget/curl <url of a file>",
    "text": "<p>download a zip file from unix prompt</p>"
  },
  {
    "commands": "cat /proc/cpuinfo",
    "text": "<p>find the number of cpu's in a system</p>"
  },
  {
    "commands": "pipe |",
    "text": "<p>how to send output of one command as input to another command</p>"
  },
  {
    "commands": "ssh <username>@<IP>",
    "text": "<p>used to connect to another system</p>"
  },
  {
    "commands": "scp f1 <username>@<IP>:/path/",
    "text": "<p>copy a file from one syatem to another system</p>"
  },
  {
    "commands": "rcp/rsh",
    "text": "<p>old commands as scp/ssh</p>"
  },
  {
    "commands": "xargs",
    "text": "<p>take the output of previous command and send it as input to next command</p>"
  },
  {
    "commands": "find /path/ -name f1.xml | xargs rm -rf",
    "text": "<p>find a file f1.xml and remote it using single command</p>"
  },
  {
    "commands": "awk/sed",
    "text": "<p>commands for text processing</p>"
  },
  {
    "commands": "ls -a",
    "text": "<p>display a hidden file</p>"
  },
  {
    "commands": "mv f1 .f1",
    "text": "<p>create a hidden file .f1 from f1</p>"
  },
  {
    "commands": "touch f1",
    "text": "<p>create an empty file</p>"
  },
  {
    "commands": "telnet IP PORT",
    "text": "<p>it is used to connect to another system just like ssh</p>"
  },
  {
    "commands": "jps",
    "text": "<p>find all the java process in the system</p>"
  },
  {
    "commands": "wc -l",
    "text": "<p>no. of lines in a file</p>"
  },


  {
    "commands": "Route Tables:",
    "text": "<p>
          <ul>
          <li>LA route table contains routes that determine
          where traffic has to be directed.</li>
          <li>Youcan have multiple routetables ina VPC, -</li>
          <li>setup a route to internet gateway for public
          setup</li>
          </ul>
      </p>"
  },
  {
    "commands": "Internet
      Gateways
      (IGW):.","text": "<p>
          <ul>
          <li>setup a route NATGateway for private setup
          Provides your public networks with a route to
          the outside world</li>
          <li>1GWis a horizontally scaled, redundantand
          highly available</li>
          <li>Only one IGW can be attached to a VPC ata
          time GW is for public subset,NAT for private.</li>
          </ul>
      </p>"
  },
  {
    "commands": " Network
      Address
      Translation
      (NAT):","text": "<p>
          <ul>
          <li>Since subnetis private, the IP addresses |
          assigned to the instances cannot be used in
          public.</li>
          <li>NAT maps the private IP addresses of inteqyal
          network to the publicaddress .</li>
          <li>NAT is provides ny IGW or NAT Gateway</li>
          </ul>
      </p>"
  },
  {
    "commands": "Elastic IP
      address","text": "<p>
          <ul>
          <li>is a static, public IPv4 address</li>
          <li> You can associate an Elastic IP address with
          any instance or network interface</li>
          <li> WithanElasticIP address, ifaninstance fails
          we can remap the address to another instance.</li>
          </ul>
      </p>"
  },
  {
    "commands": "Security
      groups:","text": "<p>
          <ul>
          <li> Security groups are set of EC2 firewall rules
          that controls the traffic for your instance.</li>
          <li> security groups can define only allow rules .
          You cannot create a rule to deny. The
          destination is always the instance on which the
          service security group is running.</li>
          <li>You can have a single security group can be
          associated with multiple instances For DENY
          rules use NACH</li>
          </ul>
      </p>"
  },
  {
    "commands": "VPC
      Peering","text": "<p>
          <ul>
          <li>A VPC peering connection allows you to route
          traffic between two VPC's . _</li>
          <li> Instances in oneVPC can communicate with
          other as if they are within the same network.</li>
          <li>You can create a VPC peering connection
          between your own VPCs, or with a VPC in
          another AWS account,</li>
          </ul>
      </p>"
  },
  {
    "commands": "Network
      Access
      Control
      Lists
      (NACL)","text": "<p>
          <ul>
          <li>Llayer of security for your VPC that acts as a za SES/SQS/SNS
          firewall for controlling traffic in and out of
          subnets.
          </li>
          <li>You set up network ACLs with rules similar to
          your security groups</li>
          <li>NAC Allows deny rules.security groups can
          not deny rules</li>
          </ul>
      </p>"
  },
  {
    "commands": "Public
      Subnet vs
      Private","text": "<p>
          <ul>
          <li> Public Subnet.is a subnet whichis directly —
          accessible from the internet</li>
          <li>Private Subnet.is a subnet which is not
          accessible from the internet.</li>
          <li>Private Subnetis only accessible from within
          the vpc i</li>
          </ul>
      </p>"
  },
  {
    "commands": "ELB - Elastic
      Load Balancer","text": "<p>
          <ul>
          <li>A load balancer accepts incoming traffic
          from clients and routes requests to EC2</li>
          <li> ELB can dynamically grow and shrinkas
          per the traffic.</li>
          <li>We can setup Application LB or network
          LB  
          ALB wutes based on http url © hHps
          nus ges are oPFER(SE™ igh or TL</li>
          </ul>
      </p>"
  },
  {
    "commands": "Auto-Scalling?",
    "text": "<p>
          <ul>
          <li>Auto-Scalling automatically adds or
          removes EC2 instances based on load</li>
          <li>We can configure auto scaling group to
          control the min& max no.of instances in  
          group</li>
          <li>When load increases, Instances will be
          created,when the load comes down
          instances will be destroyed
          </li>
          </ul>
      </p>"
  },
  {
    "commands": "Cloud watch",
    "text": "<p>
          <ul>
          <li>Real-time monitoring tool of AWS
          resources. we can getcpu,memory
          utilization , no of requests,etc .</li>
          <li>CloudWatch alarms send notifications or
          automatically make changes to the
          resources you are monitoring based on
          rules that you define</li>
          <li> We can enable CloudTrail and analize
          logs in case of any event</li>
          </ul>
      </p>"
  },
  {
    "commands": "Route 53",
    "text": "<p>
          <ul>
          <li>DNS of aws. It is used for confiquring
          website address, redirections</li>
          <li>The name is a reference to TCP or UDP
          port 53, where DNS server requests are
          addressed</li>
          <li>routeS3 will redirect the request to ELB or
          EC2 using routingalogorithm</li>
          </ul>
      </p>"
  },
  {
    "commands": "RDS(Relational database service)",
    "text": "<p>
       <ul>
         <li> (Amazon RDS) is manages service fer
          databases like myself,postgreq! sql server.</li>
        <li>It automatically patches the database
        software and manages backups as perthe
        users instructions,</li>
        <li> AWS has its own database called ,arorais
        a form of mysql.</li>
       </ul>
      </p>"
  },
  {
    "commands": "SES/SQS/SNS",
    "text": "<p>
       <ul>
          <li>Simple email service(SES) used to send
          emails to the users and you can give
          automated reolies
          </li>
        <li>you have multiple servers running
        whenever atask is done that task is removed</li>
        <li>Simple Notification Service (SNS)manages
        the delivery of messages to
        </li>
       </ul>
      </p>"
  },
  {
    "commands": "-loud Formation",
    "text": "<p>
       <ul>
          <li>Formation LC loudFormation is infrastructure-as-code for AWS resourses</li>
        <li>uses simple text file to provision, all the resources .</li>
        <li>is much Terraformis much better it works for many clouds.
        </li>

       </ul>
      </p>"
  },
  {
    "commands": " CloudTrail",
    "text": "<p>
       <ul>
          <li>Itislogging service from AWS</li>
        <li>used for auditing purposes.</li>
        <li>¥ou can lag all your API requests responses in cloudtrail</li>

       </ul>
      </p>"
  },
  {
    "commands": "Amazon Cloud-front",
    "text": "<p>
       <ul>
          <li>LCE of AWS, Le. used to deliver website content across globe</li>
        <li>Cloud-front works with $3 and EC2. ;</li>
        <li>we setup cloudfront distributions on multiple regions and upload statis content,java script files to it
        </li>

       </ul>
      </p>"
  },
  {
    "commands": "Pod",
    "text": "<p>
       <ul>
          <li>A Pod is a set of running containers,</li>
        <li>can also run optional sidecar containers
        features like logging.</li>
        <li>Pods are managed by a Deployment.</li>

       </ul>
      </p>"
  },
  {
    "commands": " Node",
    "text": "<p>
       <ul>
          <li>A node is a worker machine in Kubernetes.</li>
        <li> worker node may be a VM or physical
        machine,node has local daemons or services
        to run Pods and is managed by the control
        plane.</li>
        <li>a node has daemons like kubelet, kube-
        Proxy and all pods of deployement based on
        its taints</li>
       </ul>
      </p>"
  },
  {
    "commands": "Namespace",
    "text": "<p>
       <ul>
          <li>A logical division of feature used by
          Kubernetes to support multiple virtual
          clusters on the same physical cluster,</li>
        <li>multiple namespaces can existina one
        Cluster</li>
        <li>Names of resources need to be unique
        within a namespace, but not across
        namespaces 
        we have each dev team work in separate
        namespace.</li>

       </ul>
      </p>"
  },
  {
    "commands": "Init Container",
    "text": "<p>
       <ul>
          <li>One or more initialization containers that
          must run tocompletion before any app
          containers run.</li>
        <li> init containers run to before any app
        containers can start.</li>
        <li>Init containers run in series: each init
        container must run to completion before the
        next init container begins.</li>

       </ul>
      </p>"
  },
  {
    "commands": "Static
    pod/ Mirror
    pods/init
    pods
    ",
    "text": "<p>
       <ul>
          <li>A pod managed directly by the kubelet
          daemon on a specific node
          withouttheAPI server observingit.</li>
        <li>Static Pods are managed directly by the
        kubelet daemon ona specific node, without
        the API server</li>

       </ul>
      </p>"
  },
  {
    "commands": "Architecture",
    "text": "<p>
       <ul>
          <li>@@</li>
       </ul>
      </p>"
  },
  {
    "commands": "etcd",
    "text": "<p>
       <ul>
          <li>eted is a highly available key-value store</li>
        <li>used persistent storage of all objects</li>
        <li>deployment, pod, service Replicates ingress
        availeble in etcd</li>

       </ul>
      </p>"
  },
  {
    "commands": "Kubernetes  API",
    "text": "<p>
       <ul>
          <li>The RESTful interface to interact with etcd.</li>
        <li>desired states all stored as API objects, and
        modified via API,</li>
        <li>The API allows configuration to be  managed in a declarative way in YAML</li>
       </ul>
      </p>"
  },
  {
    "commands": "kube-scheduler",
    "text": "<p>
       <ul>
          <li> Component that watches newly created pods
          and selects a node forthem.</li>
        <li>checks resource requirements,
        hardware/software/policy constraints.</li>
        <li>affinity and anti-affinity,taints,tolerance</li>

       </ul>
      </p>"
  },
  {
    "commands" : "Kube-
    controller-
    manager",
    "text" : "<p>
    <ul>
     <li> Componentonthemaster thatruns
     controllers.</li>
     <li> each controller is a separate process.</li>
     <li>There are replication controller Node
     Admission Controller DeploymentController
     rtc</li>
    </ul>
    </p>"
  },
  {
    "commands" : " Kubelet",
    "text" : "<p>
    <ul>
     <li>An agent that runs on each node in the
     cluster,</li>
     <li>ensures that containers arerunningina pod.</li>
     <li>kubelet ensures that the containers
     described in PodSpecs YAML are running and
     healthy.</li>
    </ul>
    </p>"
  },
  {
    "commands" : " Networking",
    "text" : "<p>
    <ul>
     <li>@@</li>
    </ul>
    </p>"
  },
  {
    "commands" : "kube-proxy",
    "text" : "<p>
    <ul>
     <li>kube-proxy is a network proxy that runs on
     each node</li>
     <li>maintains network rules onnodes.</li>
     <li>Pods to each other in rough Kube-
     proxy,kubelet maintain headhof pods</li>
    </ul>
    </p>"
  },
  {
    "commands" : "RBAC (Role-
    Based
    Access
    Control)",
    "text" : "<p>
    <ul>
     <li> Manages authorization decisions.</li>
     <li> RBAC utilizes roles,which contain permission
     rules,and role buildings</li>
     <li> role,rolebinding,clusterole,clusterole
     building</li>
    </ul>
    </p>"
  },
  {
    "commands" : " Security
    Context",
    "text" : "<p>
    <ul>
     <li>The securityContext field defines privilege
     and access control settings for a Pod or
     Container
     </li>
     <li>Also including the runtime UID and GID</li>
     <li>The security context field in a pod or
     container is used to set the
     user, groups,capabilities, privilege settings,and
     security policies</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Service
    Account",
    "text" : "<p>
    <ul>
     <li>that run in aPod,</li>
     
    </ul>
    </p>"
  },
  {
    "commands" : "Account",
    "text" : "<p>
    <ul>
     <li> When processes inside Pods access the
     cluster, they are authenticated by the API
     server aS a serviceaccount,</li>
     <li> When you createa Pod, itis automatically
     assigned the default service accountin the
     same namespace.</li>
     
    </ul>
    </p>"
  },
  {
    "commands" : "Pod
    Security
    Policy",
    "text" : "<p>
    <ul>
     <li>defines a set of conditions that a Pod must
     run with</li>
     <li>this is implemented as an optional admission
     controller,</li>
     <li>Enables fine-grained authorization of Pod
     creation and updates.,</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Job",
    "text" : "<p>
    <ul>
     <li>A single or batch task</li>
     <li>Creates one or More Pod objects
     and makes sure that a specified
     Number of them successfully
     terminate,</li>
     <li>the Job tracks the successful
     completions of pods,</li>
    </ul>
    </p>"
  },
  {
    "commands" : "CronJob",
    "text" : "<p>
    <ul>
     <li>aJobthatrunsona Periodic
     schedule,</li>
     <li>Similar toa linux ina crontab file</li>
     <li>CronJob object specifies a
     schedule using the cron format</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Workload",
    "text" : "<p>
    <ul>
     <li>@@</li>
    </ul>
    </p>"
  },
  {
    "commands" : " Deployment",
    "text" : "<p>
    <ul>
     <li>aset of pods distributed among
     the nodes of a cluster,</li>
     <li>we can define replicas to specify
     no of pods</li>
     <li>Deployment are exposed or
     service in form of cluster IP,Node
     pods or tool Balanced</li>
    </ul>
    </p>"
  },
  {
    "commands" : "ConfigMap/ Secret",
    "text" : "<p>
    <ul>
     <li> ConfigMap is for storing non-
     sensitive info like environment
     variables , commandline arguments</li>
     <li>secreatis to store sensitive info
     like passwo rd,tokens,secretkeys</li>
     <li>For eg, if a deployment needs
     some env varibles and some
     Passwords, the env variables will
     go into ConfigMaps and
     passwords will go in secrets. We
     crate them before the deployment</li>
    </ul>
    </p>"
  },
  {
    "commands" : "ReplicaSet/Daemonset",
    "text" : "<p>
    <ul>
     <li>ReplicaSet, maintains theno.of
     pods pod replicas as per the
     desired state yaml file across all
     the nodes</li>
     <li>Demonsetis likea replicaset, but
     exactly one pod per node. Usually
     monitoring agents to collect logs
     on all nodes.</li>
     <li> If wehavea prometheus agent
     or ELK monitoring stack, they are
     deployed as Daemonset</li>
    </ul>
    </p>"
  },
  {
    "commands" : "StatefulSet",
    "text" : "<p>
    <ul>
     <li>StatefulSetis like replicaset but when pag
     has PVs attached to them</li>
     <li>Ifa pod dies, a replicaset recreates a new
     pod with a random name. Buta statefulset
     recreates the pod with the same name as
     before</li>
     <li>We usually use Statefulsets if there are
     databases involved like mysql and we have
     PVs,StorageClasses and Persistant Volume
     Claims
     </li>
    </ul>
    </p>"
  },
  {
    "commands" : "Service",
    "text" : "<p>
    <ul>
     <li>Lexposes an application running on Pods to
     outside</li>
     <li> The set of Pods targeted by a Serviceis
     determined by a selector.</li>
     <li>a service can be cluster IP Nodeport
     fortbalanced.</li>
    </ul>
    </p>"
  },

  {
    "commands" : "Ingress",
    "text" : "<p>
    <ul>
     <li>Ingress provides load balancing, SSL
     termination and name-based virtual hosting.</li>
     <li>ingress exposes HTTP and HTTPS routes to
     services within thecluster,</li>
     <li>Traffic routing is controlled by rules
     defined on the Ingress resource,</li>
    </ul>
    </p>"
  },

  {
    "commands" : "Storage",
    "text" : "<p>
    <ul>
     <li>@@</li>
    </ul>
    </p>"
  },

  {
    "commands" : "Persistent  Volume",
    "text" : "<p>
    <ul>
     <li>storage in the cluster beyond the life of
     Pod.</li>
     <li>PVs are used where storage can be created
     ahead of time (static Provisioning).</li>
     <li>when on-demand storage (dynamic
     Provisioning), is required
     PersistentVolumeClaims (PVCs) are used
     </li>
    </ul>
    </p>"
  },

  {
    "commands" : "Persistent
    Volume
    Claim(PVC)",
    "text" : "<p>
    <ul>
     <li>Claims storage resources defined in a
     PersistentVolumeso that itcan be mounted
     as a volume in a container.</li>
     <li> Specifies the amount of storage, how the
     storage will be accessed (read-only, read-
     write and/or exclusive)</li>
     <li>how itis reclaimed (retained, recycled or
     deleted).</li>
    </ul>
    </p>"
  },

  {
    "commands" : "Storage
    Class(PV/PVC)",
    "text" : "<p>
    <ul>
     <li>A StorageClass describes storage types
     and can map to quality-of-service levels,
     backup policies</li>
     <li>contains the fields provisioner, Parameters,
     and reclaimPolicy,used when a Persistent
     Volume to be dynamically provisioned,</li>
     <li> Users can request a particular class using
     the name of a StorageClass object.</li>
    </ul>
    </p>"
  },

  {
    "commands" : "rizontal Pod Autoscaler",
    "text" : "<p>
    <ul>
     <li>An API resource that
     automatically Scales the
     number of pod replicas
     based oncpu Utilization,</li>
     <li>HPA is used with
     Replication Controllers,
     Deployments, or ReplipliSets</li>
     <li>.for ex we can 50% cpu
     changethe replies to from 5
     to 10
     </li>
    </ul>
    </p>"
  },
  {
    "commands" : "Pod Disruption Budget",
    "text" : "<p>
    <ul>
     <li>Also known as:PDB</li>
     <li>A Pod Disruption Budget
     ensures a certain number
     Pods with an assigned label
     will not be voluntarily
     evicted at any point</li>
     <li>PDBs cannot prevent an
     involuntary disruption, but
     will count against the
     budget.'</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Taints/Tolerations",
    "text" : "<p>
    <ul>
     <li>Taints are for nodes ,
     Tolerations are for Pods</li>
     <li>Taint of a node prevents
     some pods to be scheduled
     on that node. If a pod has
     Toleration for a taint then
     that pod will be scheduled
     on that node</li>
     <li>taint & toleration has three
     properties key, value, and
     effect.</li>
    </ul>
    </p>"
  },
  {
    "commands" : "LimitRanges/ResorceQuotas",
    "text" : "<p>
    <ul>
     <li>By default, containers run
     with unlimited compute
     resources</li>
     <li>LimitRange limits
     CPU,RAM the amount of
     used by Containers or Pods.</li>
     <li>ResourceQuotas limit
     CPU,RAM for the overall
     namespace</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Label/ Annotations",
    "text" : "<p>
    <ul>
     <li>they are key/value pairs defined in
     YAML</li>
     <li>Labels are used mainly to provide the
     metadata , like name of a pod or
     deployment or sevice or ingress or
     clusterrols</li>
     <li> Labels have restrictions like it can not
     be like kubernetes.com.- Then
     annotations are used For eg to define
     the ingress controller</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Tools",
    "text" : "<p>
    <ul>
     <li>@@</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Helm Chart",
    "text" : "<p>
    <ul>
     <li>A package of YAML maintests</li>
     <li>Helm tsol used tp
     install, upgrade, rollbake application
     deployment.</li>
     <li>A single chart can be used to deploya
     full web app stack with HTTP servers,
     databases, caches.</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Kubeadm",
    "text" : "<p>
    <ul>
     <li> tool for instaling kubernetes</li>
     <li>kops,rusespray,rancher there are
     several tools to setup k8s clusters</li>
    </ul>
    </p>"
  },
  {
    "commands" : "Minikube",
    "text" : "<p>
    <ul>
     <li>tool for running Kubernetes locally.</li>
     <li>You can use Minikube to try
     kubernetes in learning environment</li>
     <li>Minikube runs a single-node cluster
     inside a VM on your computer</li>
    </ul>
    </p>"
  },
]